name: E2E PD-Disaggregation Test on Product Cluster

# Required GitHub Secrets for Product Cluster Testing:
# - KUBECONFIG_BASE64: Base64-encoded kubeconfig file for the Product cluster
#   To create: kubectl config view --flatten | base64 -w 0
# - HF_TOKEN: HuggingFace token for model download (optional)
# - HF_ENDPOINT: HuggingFace endpoint URL (optional)
# - AWS_S3_ACCESS_KEY: AWS access key ID for S3 results upload (optional, mapped to env var S3_ACCESS_KEY_ID)
# - AWS_S3_SECRET_KEY: AWS secret access key for S3 results upload (optional, mapped to env var S3_SECRET_ACCESS_KEY)
#
# Note: For Product cluster testing (SKIP_PREREQUISITE=true), setupPrerequisites() returns early
# without installing or validating any components. All required components must be pre-installed:
# - Prerequisites: cert-manager, Gateway API, Gateway controller (Istio), Gateway Inference Extension
# - MIF infrastructure: moai-inference-framework (Odin controller, etc.)
# - Preset: moai-inference-preset
# The test will only deploy heimdall and InferenceService in the test workload namespace.

on:
  workflow_dispatch:
  push:
    tags:
      - "v[0-9]+.[0-9]+.[0-9]+-rc[0-9]+"

permissions:
  packages: read
  contents: read
  checks: write

jobs:
  e2e-test:
    name: E2E PD-Disaggregation Test
    runs-on: [self-hosted, github-runner]
    concurrency:
      group: e2e-pd-test
      cancel-in-progress: false

    container:
      image: golang:1.24

    outputs:
      test_passed: ${{ steps.validate.outputs.test_passed }}

    env:
      KUBECONFIG: "/tmp/kubeconfig"
      MIF_NAMESPACE: "mif"
      ISTIO_REV: "1-28-1"
      TEST_MODEL: "Qwen/Qwen3-0.6B"
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_ENDPOINT: ${{ secrets.HF_ENDPOINT }}
      S3_ACCESS_KEY_ID: ${{ secrets.AWS_S3_ACCESS_KEY }}
      S3_SECRET_ACCESS_KEY: ${{ secrets.AWS_S3_SECRET_KEY }}
      SKIP_KIND: "true"
      SKIP_PREREQUISITE: "true"

    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          persist-credentials: false

      - name: Set up asdf
        uses: asdf-vm/actions/setup@v4

      - name: Install asdf plugins
        uses: asdf-vm/actions/plugins-add@v4

      - name: Install tools via asdf
        run: |
          asdf install kubectl
          asdf install helm

      - name: Set workload namespace
        run: |
          COMMIT_HASH_SHORT=$(echo "${{ github.sha }}" | cut -c1-7)
          echo "WORKLOAD_NAMESPACE=mif-e2e-${COMMIT_HASH_SHORT}" >> "$GITHUB_ENV"

      - name: Set up kubeconfig
        run: |
          mkdir -p ~/.kube
          # KUBECONFIG_BASE64 secret must be set in repository secrets
          # It should contain a base64-encoded kubeconfig file for the target cluster
          # To create this secret:
          # 1. Get your kubeconfig file: kubectl config view --flatten > kubeconfig.yaml
          # 2. Encode it: cat kubeconfig.yaml | base64 -w 0
          # 3. Add it as a repository secret named KUBECONFIG_BASE64
          echo "${{ secrets.KUBECONFIG_BASE64 }}" | base64 --decode > ${{ env.KUBECONFIG }}
          chmod 600 ${{ env.KUBECONFIG }}
          kubectl cluster-info

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Download dependencies
        run: go mod download

      - name: Run performance E2E test
        id: validate
        run: |
          if make test-e2e-performance; then
            echo "test_passed=true" >> "$GITHUB_OUTPUT"
          else
            echo "test_passed=false" >> "$GITHUB_OUTPUT"
            exit 1
          fi

      - name: Debug test report files
        if: always()
        run: |
          echo "Workspace: $(pwd)"
          echo "Root listing:"
          ls -la
          echo "test-reports listing:"
          ls -la test-reports || true
          echo "test/e2e/test-reports listing:"
          ls -la test/e2e/test-reports || true

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: |
            test-reports/*.xml
          check_name: E2E Test Results
          comment_mode: off
          report_individual_runs: false
          check_run_annotations: all tests, skipped tests
          report_suite_logs: any
          job_summary: true

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mif-e2e-test-reports-${{ github.run_id }}
          path: test-reports
          retention-days: 7
          if-no-files-found: warn

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [e2e-test]
    if: always()

    steps:
      - name: Check test result
        run: |
          if [ "${{ needs.e2e-test.outputs.test_passed }}" == "true" ]; then
            echo "✅ E2E tests passed. Ready for production release."
            echo "::notice::E2E tests passed. You can now create a production tag."
          else
            echo "❌ E2E tests failed. Please check the logs and fix issues before releasing."
            echo "::error::E2E tests failed. Production release blocked."
            exit 1
          fi
