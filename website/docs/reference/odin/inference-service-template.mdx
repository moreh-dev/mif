---
sidebar_position: 2
sidebar_label: Inference Service Template
title: Inference Service Template
---

# Odin InferenceService and InferenceServiceTemplate

---

## InferenceService

`InferenceService` is the primary resource for defining a distributed inference workload in Odin. It aggregates parallelism strategies, resource allocations, and template configurations to ultimately create and manage Kubernetes `Deployment` or `LeaderWorkerSet` resources.

### Key Fields

```yaml
apiVersion: odin.moreh.io/v1alpha1
kind: InferenceService
metadata:
  name: vllm-example
  namespace: my-namespace
spec:
  inferencePoolRefs:
    - name: llama-3-1b-instruct
  templateRefs:
    - name: template-hf-hub-offline
  template:
    spec:
      containers:
        - name: main
          image: 255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/quickstart/moreh-vllm:20250915.1
          command:
            - vllm
            - serve
          args:
            - meta-llama/Llama-3.2-1B-Instruct
          resources:
            limits:
              amd.com/gpu: '1'
            requests:
              amd.com/gpu: '1'
      tolerations:
        - key: 'amd.com/gpu'
          operator: 'Exists'
          effect: 'NoSchedule'
```

- `inferencePoolRefs`: The `InferencePool` where this workload is registered.
- `templateRefs`: A list of `InferenceServiceTemplate` resources in the same namespace or in the namespace where Odin is installed. These templates and `InferenceService` specifications are merged sequentially to compose the final worker specification.
- `parallelism`
  - `tensor`: The tensor parallel size.
  - `pipeline`: The pipeline parallel size.
  - `data`: The data parallel size.
  - `dataLocal`: The local data parallel size. Defaults to the data value if not specified.
  - `expert`: A boolean flag to enable expert parallelism.
- `template`: The pod template for the primary or leader workload.
- `workerTemplate`: The pod template for the worker workloads.

The `template` and `workerTemplate` fields follow the same schema as `.spec.template` in a standard Kubernetes `Deployment`. The following table describes the workload types generated based on these settings.

|     `template`     |  `workerTemplate`  |                Workload                |
| :----------------: | :----------------: | :------------------------------------: |
| :white_check_mark: |        :x:         |               Deployment               |
|        :x:         | :white_check_mark: | LeaderWorkerSet<br/>(worker-only mode) |
| :white_check_mark: | :white_check_mark: |            LeaderWorkerSet             |

The `LeaderWorkerSet` size is determined by the `parallelism` settings. The size is calculated based on the following logic:

1. If data parallelism is specified, the size is calculated as `data / dataLocal`.
2. If pipeline parallelism is specified, the size is set to the `pipeline` value.
3. If both are configured, **data parallelism takes precedence** in determining the final size.

---

## InferenceServiceTemplate

`InferenceServiceTemplate` provides reusable, modular snippets of worker configurations. These templates function as **presets** for common patterns—such as specific vLLM setups, or storage configurations—allowing you to mix and match them without duplicating complex Pod specifications across multiple services.

### Key Fields

```yaml
apiVersion: odin.moreh.io/v1alpha1
kind: InferenceServiceTemplate
metadata:
  name: template-hf-hub-offline
  namespace: mif
spec:
  template:
    spec:
      containers:
        - name: main
          env:
            - name: HF_HOME
              value: /mnt/model
            - name: HF_HUB_OFFLINE
              value: '1'
          volumeMounts:
            - name: model
              mountPath: /mnt/model
      volumes:
        - name: model
          persistentVolumeClaim:
            claimName: model
```

The `InferenceServiceTemplate` specification includes only `parallelism`, `template`, and `workerTemplate`, which are the same as those in `InferenceService`.

Templates support [Go text/template syntax](https://pkg.go.dev/text/template) and [sprig functions](https://masterminds.github.io/sprig/), allowing you to inject dynamic values from the parent `InferenceService` spec (e.g., accessing `.Spec.Parallelism.Data`).

---

## Example

This example demonstrates how to compose a complete `InferenceService` by combining three distinct templates:

- `vllm-data-parallel`: The base vLLM runtime with dynamic parallelism arguments.
- `workertemplate-decode`: A sidecar proxy for the decode role.
- `workertemplate-hf-hub-offline`: Configuration for offline Hugging Face Hub usage with persistent storage.

#### 1. Base Runtime Template (`vllm-data-parallel`)

This template sets up the main container command, environment variables, and startup logic. Note the use of `{{ "{{" }} .Spec.Parallelism.XXX {{ "}}" }}` to dynamically configure arguments.

```yaml
apiVersion: odin.moreh.io/v1alpha1
kind: InferenceServiceTemplate
metadata:
  name: vllm-data-parallel
  namespace: mif
spec:
  workerTemplate:
    spec:
      imagePullSecrets:
        - name: moreh-registry
      containers:
        - name: main
          image: 255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/quickstart/moreh-vllm:20250915.1
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
          command:
            - /bin/bash
            - -c
          args:
            - |
              set -ex

              find /dev/shm -type f -delete

              cleanup() {
                echo "Received SIGTERM, killing child processes..."
                pkill -P $$
                wait
              }
              trap cleanup SIGTERM SIGINT

              DP_SIZE={{ "{{" }} or .Spec.Parallelism.Data 1 {{ "}}" }}
              DP_LOCAL_SIZE={{ "{{" }} or .Spec.Parallelism.DataLocal 1 {{ "}}" }}
              START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_LOCAL_SIZE ))

              for ((i=0; i<DP_LOCAL_SIZE; i++)); do
                VLLM_NIXL_SIDE_CHANNEL_PORT=$((30020 + i * 2)) \
                CUDA_VISIBLE_DEVICES=$i \
                vllm serve \
                  --port $((ISVC_PORT + i)) \
                  --tensor-parallel-size {{ "{{" }} or .Spec.Parallelism.Tensor 1 {{ "}}" }} \
                  --data-parallel-size ${DP_SIZE} \
                  --data-parallel-rank $((START_RANK + i)) \
                  --data-parallel-address $(LWS_LEADER_ADDRESS) \
                  --data-parallel-rpc-port {{ "{{" }} or .Spec.Parallelism.DataRPCPort 13345 {{ "}}" }} \
                  {{ "{{" }} if .Spec.Parallelism.Expert {{ "}}" }}--enable-expert-parallel{{ "{{" }} end {{ "}}" }} \
                  $(ISVC_EXTRA_ARGS) \
                  &
              done

              wait
          env:
            - name: ISVC_PORT
              value: '8000'
            - name: ISVC_EXTRA_ARGS
              value: >-
                meta-llama/Llama-3.2-1B-Instruct
                --disable-uvicorn-access-log
                --no-enable-log-requests
            - name: UCX_TLS
              value: rocm_copy,rocm_ipc,self,sm,rc_x
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          ports:
            - name: http
              containerPort: 8000
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      tolerations:
        - key: amd.com/gpu
          operator: Exists
          effect: NoSchedule
```

#### 2. Decoder Sidecar Template (`workertemplate-decode`)

This template injects a sidecar container ("proxy") and sets specific labels and readiness probes.

```yaml
apiVersion: odin.moreh.io/v1alpha1
kind: InferenceServiceTemplate
metadata:
  name: workertemplate-decode
  namespace: mif
spec:
  workerTemplate:
    metadata:
      labels:
        heimdall/role: decode
    spec:
      imagePullSecrets:
        - name: moreh-registry
      initContainers:
        - name: proxy
          image: 255250787067.dkr.ecr.ap-northeast-2.amazonaws.com/heimdall-proxy:v0.6.0
          restartPolicy: Always
          command:
            - /bin/bash
            - -c
          args:
            - |
              set -ex

              exec /app/proxy \
                --port $(ISVC_PORT) \
                --decoder-ip $(POD_IP) \
                --decoder-port 8200 \
                --data-parallel-size {{ "{{" }} .Spec.Parallelism.Data {{ "}}" }} \
                $(ISVC_EXTRA_ARGS)
          env:
            - name: ISVC_PORT
              value: '8000'
            - name: ISVC_EXTRA_ARGS
              value: >-
                --pd-coordinator vllm/nixl
                --log-format json
                --log-level warn
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          readinessProbe:
            httpGet:
              path: /health-proxy
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
      containers:
        - name: main
          env:
            - name: ISVC_PORT
              value: '8200'
```

#### 3. Storage Template (`workertemplate-hf-hub-offline`)

This template handles the mounting of persistent volume claims for model storage.

```yaml
apiVersion: odin.moreh.io/v1alpha1
kind: InferenceServiceTemplate
metadata:
  name: workertemplate-hf-hub-offline
  namespace: mif
spec:
  workerTemplate:
    spec:
      containers:
        - name: main
          env:
            - name: HF_HOME
              value: /mnt/model
            - name: HF_HUB_OFFLINE
              value: '1'
          volumeMounts:
            - name: model
              mountPath: /mnt/model
      volumes:
        - name: model
          persistentVolumeClaim:
            claimName: model
```

#### 4. The InferenceService

This service combines the three templates. It defines the specific model arguments (`ISVC_EXTRA_ARGS`) and resource requirements, which are merged with the templates.

```yaml
apiVersion: odin.moreh.io/v1alpha1
kind: InferenceService
metadata:
  name: vllm-llama3-1b-instruct-decode-dp
spec:
  inferencePoolRefs:
    - name: heimdall
  templateRefs:
    - name: vllm-data-parallel
    - name: workertemplate-decode
    - name: workertemplate-hf-hub-offline
  parallelism:
    data: 2
  workerTemplate:
    spec:
      containers:
        - name: main
          env:
            - name: ISVC_EXTRA_ARGS
              value: >-
                meta-llama/Llama-3.2-1B-Instruct
                --disable-uvicorn-access-log
                --no-enable-log-requests
                --quantization None
                --max-model-len 8192
                --no-enable-prefix-caching
                --kv-transfer-config '{"kv_connector": "NixlConnector", "kv_role": "kv_both"}'
          resources:
            limits:
              amd.com/gpu: '2'
              mellanox/hca: '1'
            requests:
              amd.com/gpu: '2'
              mellanox/hca: '1'
```
