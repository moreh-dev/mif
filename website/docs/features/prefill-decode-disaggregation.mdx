---
sidebar_position: 2
sidebar_label: Prefill-Decode Disaggregation
title: Prefill-Decode Disaggregation
---

# Prefill-decode disaggregation

During LLM inference, computation occurs in two stages: prefill and decode. In the prefill phase, the model processes the entire input prompt to generate the first token &mdash; a highly parallel, compute-bound process. The decode phase then predicts one token at a time, reusing the growing KV cache, and is memory-bound.

Because these phases have fundamentally different characteristics, prefill-decode (PD) disaggregation executes them on separate GPU resources. The prefill runs first on compute-optimized machines, then the KV cache is transferred to memory-optimized ones for decoding. This separation allows each phase to use its optimal parallelization, batch size, and configurations, while preventing interference between concurrent requests.

PD disaggregation can improve key metrics such as time to first token (TTFT) and time per output token (TPOT) &mdash; since TTFT depends on prefill and TPOT on decode, dedicated optimization for each leads to better overall performance. However, because it also introduces communication overhead, which may negatively affect TTFT, PD disaggregation should be applied judiciously to ensure net efficiency gains.

---

## Key features

- The **Heimdall** scheduler runs prefill-only and decode-only instances separately, allowing each to scale independently and managing request routing between them.
- The framework can automatically determine whether to apply PD disaggregation and how to scale each phase according to defined service level objectives (SLOs).
- Moreh vLLM is optimized to efficiently execute both prefill and decode phases of various models on AMD MI200 and MI300 series GPUs. It applies distinct parallelization and optimization strategies tailored to prefill-only and decode-only instances.

---

## Manual configuration of PD disaggregation

To enable PD disaggregation, configure the prefill pod separately from the decode pod in the **Odin** inference service.

```yaml inference-service-values.yaml

...
decode: ...

prefill:
  enabled: true

  replicas: 4

  resources:
    requests:
      amd.com/gpu: '2'
    limits:
      amd.com/gpu: '2'

  extraArgs: ...
```

Within the `decode` and `prefill` section, you can independently configure not only the number of replicas (`replicas`) but also the type and number of GPUs (`resources`) and the argument passed to the inference engine (`extraArgs`). Make suire that the options specified in the `extraArgs` field of the `decode` or `prefill` sections do not duplicate those defined in the global `extraArgs`.

Additionally, in the **Heimdall** scheduler, you must define separate scheduling profiles for prefill and decode as shown below.

```yaml heimdall-values.yaml
...
config:
  ...
  plugins:
    - type: pd-profile-handler
    - type: prefill-filter
    - type: decode-filter
    ...
  schedulingProfiles:
    - name: prefill
      plugins:
        - pluginRef: prefill-filter
        - pluginRef: queue-scorer
        - pluginRef: max-score-picker
    - name: decode
      plugins:
        - pluginRef: decode-filter
        - pluginRef: queue-scorer
        - pluginRef: max-score-picker
...
```

---

:::info
**Benchmark Results**: See [PD Disaggregation (Llama 70B)](/benchmarking/pd-disaggregation-llama-70b.mdx) for performance comparisons.
:::
